import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import load_model
import matplotlib.pyplot as plt
import numpy as np
import os
import cv2
import os
import PIL
import time
from IPython import display
print("available gpu", len(tf.config.list_physical_devices('GPU')))
##################################Global parameters##############################################
mse = tf.keras.metrics.MeanSquaredError()
mae = tf.keras.metrics.MeanAbsoluteError()
LEARNING_RATE_GEN = 1e-5
BUFFER_SIZE = 60000
BATCH_SIZE = 32
########################
##########Dataset##############################################
# load H-voice datasets( available at  https://data.mendeley.com/datasets/k47yd3m28w/4)
path=".\Training_original\Training_original"
def create_dataset(img_folder):
  histo_data=[]
  IMG_WIDTH_HIST=150
  IMG_HEIGHT_HIST=150
  for file in os.listdir(img_folder):
        hist_path= os.path.join(img_folder,  file)
        hist= cv2.imread( hist_path)
        hist=cv2.resize(hist, (IMG_HEIGHT_HIST, IMG_WIDTH_HIST))
        hist=np.array(hist)
        hist = hist.astype('float32')
        hist /= 255

        histo_data.append(hist)
     


  return  histo_data
histo_data =create_dataset(path)



train_dataset = tf.data.Dataset.from_tensor_slices((histo_data)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

##################################Generator##############################################



# Create the generator
def create_generator(latent_dim):

    generator =keras.Sequential(
    [keras.Input(shape=(latent_dim,)),
        layers.Dense(15 * 15 * 128),
        layers.BatchNormalization(),
        layers.LeakyReLU(),
        layers.Reshape((15, 15, 128)),
        
        layers.Conv2DTranspose(64, kernel_size=5, strides=2,output_padding=1),
        layers.LeakyReLU(),
        layers.Dropout(0.2),
        layers.Conv2DTranspose(32, kernel_size=5, strides=2),
        layers.LeakyReLU(),
        layers.Dropout(0.2),
        layers.Conv2DTranspose(32, kernel_size=5, strides=2,output_padding=1),
        layers.LeakyReLU(),
        layers.Dropout(0.2),

        layers.Conv2DTranspose(3, kernel_size=5, strides=1,),
      
    ],
    name="generator",
    )
    return generator

latent_dim = 150

generator = create_generator((latent_dim))
generator.summary()


##################################Discriminator##############################################
#Load the Deep4SNET trained model 

def create_discriminator():
    model = '.\deep4snetModel\model_Deep4SNet.h5'
    weights_model = '.\deep4snetModel\weights_Deep4SNet.h5'
    model = load_model(model)
    model.load_weights(weights_model)
    return model

discriminator = create_discriminator()
discriminator.summary()


##################################Loss functions##############################################
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

generator_optimizer = tf.keras.optimizers.Adam(1e-4)
discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)


##################################GAN Training##############################################


EPOCHS = 2000

num_examples_to_generate = 1
seed = tf.random.normal([num_examples_to_generate, latent_dim])
checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                 generator=generator,
                                 discriminator=discriminator)
@tf.function
def train_step(images):
    # Sample random points in the latent space
    noise = tf.random.normal([BATCH_SIZE, latent_dim])
    print(generator.trainable_variables)
    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        
      generated_images = generator(noise, training=True)
      
      real_output = discriminator(images, training=True)
      fake_output = discriminator(generated_images, training=True)
       
      gen_loss = generator_loss(fake_output)
      print(gen_loss)
      disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

def generate_and_save_images(model, epoch, test_input):
  # Notice `training` is set to False.
  # This is so all layers run in inference mode (batchnorm).
  predictions = model(test_input, training=False)

  fig = plt.figure(figsize=(10, 10))
  plt.imshow(predictions[0, :, :, :],cmap='gray')
  plt.axis('off')

  plt.savefig('image2_at_epoch_{:04d}.png'.format(epoch))
  plt.show()
  

        
    
def train(dataset, epochs):
    for epoch in range(epochs):
      start = time.time()
  
      for image_batch in dataset:
        train_step(image_batch)
  
      if epoch % 50 == 0:
        generate_and_save_images(generator,epoch + 1,seed)
        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))
        """
        display.clear_output(wait=True)
      generate_and_save_images(generator,
                               epoch + 1,
                               seed)
  
      # Save the model every 15 epochs
      if (epoch + 1) % 15 == 0:
        checkpoint.save(file_prefix = checkpoint_prefix)
  
      print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))
      """
    # Generate after the final epoch
    display.clear_output(wait=True)
    generate_and_save_images(generator,
                             epochs,
                             seed)

  
class GAN(keras.Model):
    def __init__(self, generator, discriminator, **kwargs):
        # MelGAN trainer class.
        super().__init__(**kwargs)
        self.generator = generator
        self.discriminator = discriminator

# Start training.
generator = create_generator((latent_dim))
discriminator = create_discriminator()
gan = GAN(generator, discriminator)
train(train_dataset,EPOCHS)








